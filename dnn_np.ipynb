{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from util import *\n",
    "import activation_np as active \n",
    "from gradient_check import *\n",
    "import pdb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    def __init__(self, num_epoch=1000, batch_size=100, learning_rate=0.0005, momentum_rate=0.9, epochs_to_draw=10, reg=0.00015, num_train=1000, visualize=True):\n",
    "        self.num_epoch = num_epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum_rate = momentum_rate\n",
    "        self.epochs_to_draw = epochs_to_draw\n",
    "        self.reg = reg\n",
    "        self.num_train = num_train\n",
    "        self.visualize = visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLASS LAYER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    def __init__(self, w_shape, activation, reg = 1e-5):\n",
    "        \"\"\"__init__\n",
    "\n",
    "        :param w_shape: create w with shape w_shape using normal distribution\n",
    "        :param activation: string, indicating which activation function to be used\n",
    "        \"\"\"\n",
    "        \n",
    "        mean = 0\n",
    "        std = 1\n",
    "        self.w = np.random.normal(0, np.sqrt(2./np.sum(w_shape)), w_shape)\n",
    "        self.activation = activation\n",
    "        self.reg = reg\n",
    "    def forward(self, x):\n",
    "        \"\"\"forward\n",
    "        This function compute the output of this layer\n",
    "        \n",
    "        :param x: input\n",
    "        \"\"\"\n",
    "        # [TODO 1.2]\n",
    "        result = self.w@x\n",
    "        \n",
    "        # Compute different types of activation\n",
    "        if (self.activation == 'sigmoid'):\n",
    "            result = active.sigmoid(x)\n",
    "        elif (self.activation == 'relu'):\n",
    "            result = active.reLU(x)\n",
    "        elif (self.activation == 'tanh'):\n",
    "            result = active.tanh(x)\n",
    "        elif (self.activation == 'softmax'):\n",
    "            result = active.softmax(x)\n",
    "\n",
    "        self.output = result\n",
    "        return result\n",
    "\n",
    "    def backward(self, x, delta_dot_w_prev):\n",
    "        \"\"\"backward\n",
    "        This function compute the gradient of the loss function with respect to the parameter (w) of this layer\n",
    "\n",
    "        :param x: input of the layer\n",
    "        :param delta_dot_w_prev: delta^(l+1) dot product with w^(l+1)T, computed from the next layer (in feedforward direction) or previous layer (in backpropagation direction)\n",
    "        \"\"\"\n",
    "        # [TODO 1.2]\n",
    "        if(self.activation == 'sigmoid'):\n",
    "            w_grad = active.sigmoid_grad(x) \n",
    "        \n",
    "        elif(self.activation == 'tanh'):\n",
    "           w_grad = active.tanh_grad(x)\n",
    "\n",
    "        elif(self.activation == 'relu'):\n",
    "           w_grad = active.reLU_grad(x) \n",
    "\n",
    "        # [TODO 1.4] Implement L2 regularization on weights here\n",
    "        w_grad +=  0\n",
    "        return w_grad, delta.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLASE NEURALNET\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(object):\n",
    "    def __init__(self, num_class=2, reg = 1e-5):\n",
    "        self.layers = []\n",
    "        self.momentum = []\n",
    "        self.reg = reg\n",
    "        self.num_class = num_class\n",
    "        \n",
    "    def add_linear_layer(self, w_shape, activation):\n",
    "        \"\"\"add_linear_layer\n",
    "\n",
    "        :param w_shape: create w with shape w_shape using normal distribution\n",
    "        :param activation: string, indicating which activation function to be used\n",
    "        \"\"\"\n",
    "        if(len(self.layers) != 0):\n",
    "            if(w_shape[0] != self.layers[-1].w.shape[-1]):\n",
    "                raise ValueError(\"Shape does not match between the added layer and previous hidden layer.\")\n",
    "\n",
    "        if(activation == 'sigmoid'):\n",
    "            self.layers.append(Layer(w_shape, 'sigmoid', self.reg))\n",
    "        elif(activation == 'relu'):\n",
    "            self.layers.append(Layer(w_shape, 'relu', self.reg)) \n",
    "        elif(activation == 'tanh'):\n",
    "            self.layers.append(Layer(w_shape, 'tanh', self.reg))\n",
    "        elif(activation == 'softmax'):\n",
    "            self.layers.append(Layer(w_shape, 'softmax', self.reg))\n",
    "        self.momentum.append(np.zeros_like(self.layers[-1].w))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"forward\n",
    "\n",
    "        :param x: input\n",
    "        \"\"\"\n",
    "        all_x = [x]\n",
    "        for layer in self.layers:\n",
    "            all_x.append(layer.forward(all_x[-1]))\n",
    "        \n",
    "        return all_x\n",
    "\n",
    "\n",
    "    def compute_loss(self, y, y_hat):\n",
    "        \"\"\"compute_loss\n",
    "        Compute the average cross entropy loss using y (label) and y_hat (predicted class)\n",
    "\n",
    "        :param y:  the label, the actual class of the samples. e.g. 3-class classification with 9 data samples y = [0 0 0 1 1 1 2 2 2]\n",
    "        :param y_hat: the propabilities that the given samples belong to class 1\n",
    "        \"\"\"\n",
    "\n",
    "        # [TODO 1.3]\n",
    "        # Estimating cross entropy loss from y_hat and y \n",
    "        data_loss = -1*(y*np.log(y_hat))\n",
    "        data_loss=np.mean(data_loss)\n",
    "        # Estimating regularization loss from all layers\n",
    "        reg_loss = 0.0\n",
    "        data_loss += reg_loss\n",
    "\n",
    "        return data_loss\n",
    "    \n",
    "    def backward(self, y, all_x):\n",
    "        \"\"\"backward\n",
    "\n",
    "        :param y: the label, the actual class of the samples. e.g. 3-class classification with 9 data samples y = [0 0 0 1 1 1 2 2 2]\n",
    "        :param all_x: input data and activation from every layer\n",
    "        \"\"\"\n",
    "        \n",
    "        # [TODO 1.5] Compute delta factor from the output\n",
    "        delta = (all_x[-1]-y)\n",
    "        delta /= y.shape[0]\n",
    "        \n",
    "        # [TODO 1.5] Compute gradient of the loss function with respect to w of softmax layer, use delta from the output\n",
    "        grad_last = np.matmul([all_x[-2]],delta)\n",
    "\n",
    "        grad_list = []\n",
    "        grad_list.append(grad_last)\n",
    "        \n",
    "        for i in range(len(self.layers) - 1)[::-1]:\n",
    "            prev_layer = self.layers[i+1]\n",
    "            layer = self.layers[i]\n",
    "            x = all_x[i]\n",
    "\t    # [TODO 1.5] Compute delta_dot_w_prev factor for previous layer (in backpropagation direction)\n",
    "\t    # delta_prev: delta^(l+1), in the start of this loop, delta_prev is also delta^(L) or delta_last\n",
    "\t    # delta_dot_w_prev: delta^(l+1) dot product with w^(l+1)T\n",
    "            delta_dot_w_prev = delta@grad_list[i].T\n",
    "\t    # Use delta_dot_w_prev to compute delta factor for the next layer (in backpropagation direction)\n",
    "            grad_w, delta = layer.backward(x, delta_dot_w_prev)\n",
    "            grad_list.append(grad_w.copy())\n",
    "\n",
    "        grad_list = grad_list[::-1]\n",
    "        return grad_list\n",
    "    \n",
    "    def update_weight(self, grad_list, learning_rate):\n",
    "        \"\"\"update_weight\n",
    "        Update w using the computed gradient\n",
    "\n",
    "        :param grad: gradient computed from the loss\n",
    "        :param learning_rate: float, learning rate\n",
    "        \"\"\"\n",
    "        for i in range(len(self.layers)):\n",
    "            layer = self.layers[i]\n",
    "            grad = grad_list[i]\n",
    "            layer.w = layer.w - learning_rate * grad\n",
    "    \n",
    "    \n",
    "    def update_weight_momentum(self, grad_list, learning_rate, momentum_rate):\n",
    "        \"\"\"update_weight_momentum\n",
    "        Update w using SGD with momentum\n",
    "\n",
    "        :param grad: gradient computed from the loss\n",
    "        :param learning_rate: float, learning rate\n",
    "        :param momentum_rate: float, momentum rate\n",
    "        \"\"\"\n",
    "        for i in range(len(self.layers)):\n",
    "            layer = self.layers[i]\n",
    "            self.momentum[i] = self.momentum[i]*momentum_rate + learning_rate*grad_list[i]\n",
    "            layer.w = layer.w - self.momentum[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(y_hat, test_y):\n",
    "    \"\"\"test\n",
    "    Compute the confusion matrix based on labels and predicted values \n",
    "\n",
    "    :param y_hat: predicted probabilites, output of classifier.feed_forward\n",
    "    :param test_y: test labels\n",
    "    \"\"\"\n",
    "    if (y_hat.ndim == 2):\n",
    "        y_hat = np.argmax(y_hat, axis=1)\n",
    "    num_class = np.unique(test_y).size\n",
    "    confusion_mat = np.zeros((num_class, num_class))\n",
    "\n",
    "    for i in range(num_class):\n",
    "        class_i_idx = test_y == i\n",
    "        num_class_i = np.sum(class_i_idx)\n",
    "        y_hat_i = y_hat[class_i_idx]\n",
    "        for j in range(num_class):\n",
    "            confusion_mat[i,j] = 1.0*np.sum(y_hat_i == j)/num_class_i\n",
    "\n",
    "    np.set_printoptions(precision=2)\n",
    "    print('Confusion matrix:')\n",
    "    print(confusion_mat)\n",
    "    print('Diagonal values:')\n",
    "    print(confusion_mat.flatten()[0::(num_class+1)])\n",
    "\n",
    "    \n",
    "def unit_test_layer(your_layer):\n",
    "    \"\"\"unit test layer\n",
    "\n",
    "    This function is used to test layer backward and forward for a random datapoint\n",
    "    error < 1e-8 - you should be happy\n",
    "    error > e-3  - probably wrong in your implementation\n",
    "    \"\"\"\n",
    "    # generate a random data point\n",
    "    x_test = np.random.randn(1, your_layer.w.shape[0])\n",
    "    layer_sigmoid = Layer(your_layer.w.shape, your_layer.activation, reg = 0.0)\n",
    "\n",
    "    #randomize the partial derivative of the cost function w.r.t the next layer    \n",
    "    delta_prev = np.ones((1,your_layer.w.shape[1]))\n",
    "    \n",
    "    # evaluate the numerical gradient of the layer\n",
    "    numerical_grad = eval_numerical_gradient(layer_sigmoid, x_test, delta_prev, False)\n",
    "\n",
    "    #evaluate the gradient using back propagation algorithm\n",
    "    layer_sigmoid.forward(x_test)\n",
    "    w_grad, delta = layer_sigmoid.backward(x_test, delta_prev)\n",
    "\n",
    "    #print out the relative error\n",
    "    error = rel_error(w_grad, numerical_grad)\n",
    "    print(\"Relative error between numerical grad and function grad is: %e\" %error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BATCH TRAIN MINI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minibatch_train(net, train_x, train_y, cfg):\n",
    "    \"\"\"minibatch_train\n",
    "    Train your neural network using minibatch strategy\n",
    "\n",
    "    :param net: NeuralNet object\n",
    "    :param train_x: numpy tensor, train data\n",
    "    :param train_y: numpy tensor, train label\n",
    "    :param cfg: Config object\n",
    "    \"\"\"\n",
    "    # [TODO 1.6] Implement mini-batch training\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BATCH TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_train(net, train_x, train_y, cfg):\n",
    "    \"\"\"batch_train\n",
    "    Train the neural network using batch SGD\n",
    "\n",
    "    :param net: NeuralNet object\n",
    "    :param train_x: numpy tensor, train data\n",
    "    :param train_y: numpy tensor, train label\n",
    "    :param cfg: Config object\n",
    "    \"\"\"\n",
    "\n",
    "    train_set_x = train_x[:cfg.num_train].copy()\n",
    "    train_set_y = train_y[:cfg.num_train].copy()\n",
    "    train_set_y = create_one_hot(train_set_y, net.num_class)\n",
    "    all_loss = []\n",
    "\n",
    "    for e in range(cfg.num_epoch):\n",
    "        all_x = net.forward(train_set_x)\n",
    "        y_hat = all_x[-1]\n",
    "        loss = net.compute_loss(train_set_y, y_hat)\n",
    "        grads = net.backward(train_set_y, all_x)\n",
    "        net.update_weight(grads, cfg.learning_rate)\n",
    "\n",
    "        all_loss.append(loss)\n",
    "\n",
    "        if (e % cfg.epochs_to_draw == cfg.epochs_to_draw-1):\n",
    "            if (cfg.visualize):\n",
    "                y_hat = net.forward(train_x[0::3])[-1]\n",
    "                visualize_point(train_x[0::3], train_y[0::3], y_hat)\n",
    "            plot_loss(all_loss, 2)\n",
    "            plt.show()\n",
    "            plt.pause(0.01)\n",
    "\n",
    "        print(\"Epoch %d: loss is %.5f\" % (e+1, loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BAT_CLASSIFICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bat_classification():\n",
    "    # Load data from file\n",
    "    # Make sure that bat.dat is in data/\n",
    "    train_x, train_y, test_x, test_y = get_bat_data()\n",
    "    train_x, _, test_x = normalize(train_x, train_x, test_x)    \n",
    "\n",
    "    test_y  = test_y.flatten()\n",
    "    train_y = train_y.flatten()\n",
    "    num_class = (np.unique(train_y)).shape[0]\n",
    "\n",
    "    # Pad 1 as the third feature of train_x and test_x\n",
    "    train_x = add_one(train_x) \n",
    "    test_x = add_one(test_x)\n",
    "\n",
    "    # Define hyper-parameters and train-related parameters\n",
    "    cfg = Config(num_epoch=1000, learning_rate=0.001, num_train=train_x.shape[0])\n",
    "\n",
    "    # Create NN classifier\n",
    "    num_hidden_nodes = 100\n",
    "    num_hidden_nodes_2 = 100\n",
    "    num_hidden_nodes_3 = 100\n",
    "    net = NeuralNet(num_class, cfg.reg)\n",
    "    net.add_linear_layer((train_x.shape[1],num_hidden_nodes), 'relu')\n",
    "    net.add_linear_layer((num_hidden_nodes, num_hidden_nodes_2), 'relu')\n",
    "    net.add_linear_layer((num_hidden_nodes_2, num_hidden_nodes_3), 'relu')\n",
    "    net.add_linear_layer((num_hidden_nodes_3, num_class), 'softmax')\n",
    "    \n",
    "    #Sanity check - train in small number of samples to see the overfitting problem- the loss value should decrease rapidly\n",
    "    #cfg.num_train = 500\n",
    "    #batch_train(net, train_x, train_y, cfg)\n",
    "\n",
    "    #Batch training - train all dataset\n",
    "    #batch_train(net, train_x, train_y, cfg)\n",
    "\n",
    "    #Minibatch training - training dataset using Minibatch approach\n",
    "    minibatch_train(net, train_x, train_y, cfg)\n",
    "    \n",
    "    y_hat = net.forward(test_x)[-1]\n",
    "    test(y_hat, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST_CLASSIFICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_classification():\n",
    "    # Load data from file\n",
    "    # Make sure that fashion-mnist/*.gz is in data/\n",
    "    train_x, train_y, val_x, val_y, test_x, test_y = get_mnist_data(1)\n",
    "    train_x, val_x, test_x = normalize(train_x, train_x, test_x)    \n",
    "    \n",
    "    num_class = (np.unique(train_y)).shape[0]\n",
    "\n",
    "    # Pad 1 as the third feature of train_x and test_x\n",
    "    train_x = add_one(train_x)\n",
    "    val_x = add_one(val_x)\n",
    "    test_x = add_one(test_x)\n",
    "\n",
    "    # Define hyper-parameters and train-related parameters\n",
    "    cfg = Config(num_epoch=300, learning_rate=0.001, batch_size=200, num_train=train_x.shape, visualize=False)\n",
    "\n",
    "    # Create NN classifier\n",
    "    num_hidden_nodes = 100\n",
    "    num_hidden_nodes_2 = 100\n",
    "    num_hidden_nodes_3 = 100\n",
    "    net = NeuralNet(num_class, cfg.reg)\n",
    "    net.add_linear_layer((train_x.shape[1],num_hidden_nodes), 'relu')\n",
    "    net.add_linear_layer((num_hidden_nodes, num_hidden_nodes_2), 'relu')\n",
    "    net.add_linear_layer((num_hidden_nodes_2, num_hidden_nodes_3), 'relu')\n",
    "    net.add_linear_layer((num_hidden_nodes_3, num_class), 'softmax')\n",
    "     \n",
    "    #Minibatch training - training dataset using Minibatch approach\n",
    "    minibatch_train(net, train_x, train_y, cfg)\n",
    "    \n",
    "    y_hat = net.forward(test_x)[-1]\n",
    "    test(y_hat, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    np.random.seed(2017)\n",
    "    \n",
    "    #numerical check for your layer feedforward and backpropagation\n",
    "    your_layer = Layer((60, 100), 'sigmoid')\n",
    "    unit_test_layer(your_layer)\n",
    "\n",
    "    plt.ion()\n",
    "    bat_classification()\n",
    "    mnist_classification()\n",
    "\n",
    "    pdb.set_trace()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('pythonProject')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ab47d8f26d0de71b951a80b6ec953abb660ed114394f9d340a18606f921977ea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
